{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset and reviewing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Shuffled_CSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the header\n",
    "Header = df.head(5)\n",
    "print(Header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "null_Values = df.isnull().sum()\n",
    "print(null_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the languages \n",
    "from langdetect import detect\n",
    "for i in range (len(df[\"text\"])):\n",
    "    language = detect(df['text'][i])\n",
    "    if language != 'en':\n",
    "        print(language)\n",
    "        print(df['text'][i])\n",
    "        print(i)\n",
    "        df.drop(i, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\",\".join(stopwords.words(\"english\"))\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the remove_stop_words function\n",
    "def remove_stop_words(x):\n",
    "    return \" \".join([word for word in str(x).split() if word.lower() not in stop_words])\n",
    "\n",
    "# Apply the function to create a new column 'filtered_text'\n",
    "df['filtered_text'] = df['text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filtered_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bar_plot = df[\"stars\"].value_counts().sort_index() \\\n",
    "    .plot(kind='bar',\n",
    "           title=\"Count of reviews by stars\",\n",
    "           figsize=(10,5))\n",
    "\n",
    "Bar_plot.set_xlabel('Review Stars')\n",
    "Bar_plot.set_ylabel('Number of reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 for positive, 1 for neutral, 3 for negative\n",
    "df[\"Reviews\"] = df[\"stars\"].apply(lambda score: 2 if score >= 4 else 1 if score == 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Reviews\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_DataFrame = df[['filtered_text','Reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bar_plot = df[\"Reviews\"].value_counts().sort_index() \\\n",
    "    .plot(kind='bar',\n",
    "           title=\"Comparission of postive and negative reviews\",\n",
    "           figsize=(10,5))\n",
    "\n",
    "Bar_plot.set_xlabel('Type of the review')\n",
    "Bar_plot.set_ylabel('Number of reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_DataFrame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset as Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing datasets\n",
    "train_Text_Data, test_Text_Data, train_label_Data, test_labels_Data = train_test_split(New_DataFrame['filtered_text'], New_DataFrame['Reviews'], test_size=0.2,stratify=New_DataFrame[\"Reviews\"], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_Text_Data.size)\n",
    "print(test_labels_Data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Pre-Trained model with the tokenizer\n",
    "model_Name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_Name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the input text data\n",
    "train_text_encodings = tokenizer(train_Text_Data.tolist(), truncation=True, padding=True)\n",
    "test_text_encodings = tokenizer(test_Text_Data.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels and encodings in dataset objects\n",
    "train_Dataset_Object = TensorDataset(torch.tensor(train_text_encodings['input_ids']),\n",
    "                              torch.tensor(train_text_encodings['attention_mask']),\n",
    "                              torch.tensor(train_label_Data.tolist()))\n",
    "test_Dataset_Object = TensorDataset(torch.tensor(test_text_encodings['input_ids']),\n",
    "                             torch.tensor(test_text_encodings['attention_mask']),\n",
    "                             torch.tensor(test_labels_Data.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train loaders using the dataset objects\n",
    "train_Data_Loader = DataLoader(train_Dataset_Object, batch_size=4, shuffle=True)\n",
    "test_Data_Loader = DataLoader(test_Dataset_Object, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the device for GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              weight_decay=0.01,\n",
    "                              lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Creating a progress bar for batches\n",
    "    progress_bar = tqdm(enumerate(train_Data_Loader, 1), total=len(train_Data_Loader), desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        input_ids, attention_mask, Reviews = map(lambda x: x.to(device), batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=Reviews)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculating average training loss for each epoch\n",
    "        avg_epoch_loss = epoch_loss / batch_idx\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        progress_bar.set_postfix({'Training Loss': avg_epoch_loss})\n",
    "\n",
    "    # Calculate and display the total training time at the end of each epoch\n",
    "    elapsed_Total_Time = time.time() - start_time\n",
    "    total_minutes = elapsed_Total_Time / 60\n",
    "    print(f'Total Training Time for Epoch {epoch + 1}: {elapsed_Total_Time:.2f} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the local pc\n",
    "model.save_pretrained('Models')\n",
    "tokenizer.save_pretrained('Tokens')\n",
    "\n",
    "print(\"Model saved......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model to do predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved fine-tuned model to get predictions\n",
    "tokenizer = AutoTokenizer.from_pretrained('Tokens')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Models')\n",
    "\n",
    "# Setting the device for GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def get_predictions(review):\n",
    "    # Tokenizing the user review\n",
    "    input_Data = tokenizer(review,truncation=True, padding=True,return_tensors='pt',)\n",
    "    input_ids = input_Data['input_ids'].to(device)\n",
    "    attention_mask = input_Data['attention_mask'].to(device)\n",
    "\n",
    "    # Getting the model prediction according to the user review\n",
    "    with torch.no_grad():\n",
    "        Prediction = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Getting the predicted labels\n",
    "    logits = Prediction.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting user reviews\n",
    "user_Review1 = \"Wow that is delicious\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Prediction = get_predictions(user_Review1)\n",
    "if review_Prediction == 2:\n",
    "    print(\"Positive Feedback.\")\n",
    "elif review_Prediction == 0:\n",
    "    print(\"Negative Feedback.\")\n",
    "else:\n",
    "    print(\"Neutral Feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting user reviews 2\n",
    "user_Review2 = \"Too spicy for my taste\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Prediction = get_predictions(user_Review2)\n",
    "if review_Prediction == 2:\n",
    "    print(\"Positive Feedback.\")\n",
    "elif review_Prediction == 0:\n",
    "    print(\"Negative Feedback.\")\n",
    "else:\n",
    "    print(\"Neutral Feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting user reviews 3\n",
    "user_Review3 = \"Exceptional service\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Prediction = get_predictions(user_Review3)\n",
    "if review_Prediction == 2:\n",
    "    print(\"Positive Feedback.\")\n",
    "elif review_Prediction == 0:\n",
    "    print(\"Negative Feedback.\")\n",
    "else:\n",
    "    print(\"Neutral Feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting user reviews 4\n",
    "user_Review4 = \"Causal atmosphere, average food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Prediction = get_predictions(user_Review4)\n",
    "if review_Prediction == 2:\n",
    "    print(\"Positive Feedback.\")\n",
    "elif review_Prediction == 0:\n",
    "    print(\"Negative Feedback.\")\n",
    "else:\n",
    "    print(\"Neutral Feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting user reviews 5\n",
    "user_Review5 = \"Music was too loud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Prediction = get_predictions(user_Review5)\n",
    "if review_Prediction == 2:\n",
    "    print(\"Positive Feedback.\")\n",
    "elif review_Prediction == 0:\n",
    "    print(\"Negative Feedback.\")\n",
    "else:\n",
    "    print(\"Neutral Feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.eval()\n",
    "review_Predictions = []\n",
    "ground_Truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_Data_Loader:\n",
    "        input_ids, attention_mask, Reviews = map(lambda x: x.to(device), batch)\n",
    "        prediction_Results = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = prediction_Results.logits\n",
    "        review_Predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        ground_Truth.extend(Reviews.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Data_Accuracy = accuracy_score(ground_Truth, review_Predictions)\n",
    "print(f\"The test data accuracy is : {test_Data_Accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the cunfusion matrix\n",
    "confusion = confusion_matrix(ground_Truth, review_Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLotting the confusion matrix in a heat map\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative Reviews',\"Neutral Reviews\", 'Positive Reviews'],yticklabels=['Negative Reviews',\"Neutral Reviews\", 'Positive Reviews'])\n",
    "plt.title('Model Results')\n",
    "plt.xlabel('Predicted Reviews')\n",
    "plt.ylabel('Ground Truth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ground_Truth, review_Predictions,target_names=['Negative reviews','Neutral Reviews', 'Postive reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
